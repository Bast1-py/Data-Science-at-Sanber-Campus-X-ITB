{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34469a68",
   "metadata": {},
   "source": [
    "# Pembersihan Data\n",
    "Pengecekan Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2aa04b",
   "metadata": {},
   "source": [
    "Outlier atau pencilan adalah data yang signifikan berbeda dari sebagian besar data lainnya dalam satu set. Deteksi outlier merupakan langkah kritis dalam analisis data dan statistika, karena outlier dapat memiliki dampak besar pada hasil analisis statistik. Berikut adalah beberapa pendekatan umum untuk deteksi outlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeed36f",
   "metadata": {},
   "source": [
    "# Metode Z-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef90050",
   "metadata": {},
   "source": [
    "Metode ini mengukur seberapa jauh suatu nilai dari rerata dalam satuan deviasi standar.\n",
    "\n",
    "Outlier diidentifikasi jika nilai Z-Score melebihi batas tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35088b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "z_scores = stats.zscore(data)\n",
    "outliers = (z_scores > 3) | (z_scores < -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6a01a",
   "metadata": {},
   "source": [
    "# IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13b7ca",
   "metadata": {},
   "source": [
    "Menggunakan jangkauan antarkuartil (Q3 - Q1) sebagai ukuran sebaran data. Nilai di luar rentang ini dianggap sebagai outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cafa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed1a2f",
   "metadata": {},
   "source": [
    "# DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb76944",
   "metadata": {},
   "source": [
    "Algoritma clustering yang mengidentifikasi kelompok padat data dan mengisolasi noise. Poin yang kurang terhubung dianggap sebagai outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed71504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
    "outliers = (dbscan.fit_predict(data) == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc51a34",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec66a31",
   "metadata": {},
   "source": [
    "Algoritma berbasis pohon yang mengukur seberapa mudah nilai dapat diisolasi dari data lainnya. Skor yang rendah menunjukkan kemungkinan outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75132287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "isolation_forest = IsolationForest(contamination=0.05)\n",
    "outliers = isolation_forest.fit_predict(data) == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0cf54",
   "metadata": {},
   "source": [
    "# Penanganan Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a732f",
   "metadata": {},
   "source": [
    "Penghapusan Outlier:\n",
    "Metode ini melibatkan penghapusan nilai-nilai outlier dari dataset. Harus dilakukan dengan hati-hati, karena dapat mengubah statistik deskriptif dan distribusi data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b941eeb",
   "metadata": {},
   "source": [
    "# Metode Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = stats.zscore(data)\n",
    "data_no_outliers = data[(z_scores < 3) & (z_scores > -3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c57670",
   "metadata": {},
   "source": [
    "Transformasi Data: Menggunakan transformasi matematika pada data untuk mengurangi dampak outlier. Misalnya, transformasi logaritma dapat membantu meredakan efek outlier pada distribusi data yang mendekati distribusi log-normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f472f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformasi logaritma pada data\n",
    "data_transformed = np.log1p(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71667e1f",
   "metadata": {},
   "source": [
    "Imputasi Nilai: Menggantikan nilai outlier dengan nilai yang lebih sesuai berdasarkan strategi tertentu (misalnya, median atau mean). Menggunakan nilai yang lebih stabil daripada nilai outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputasi outlier dengan median\n",
    "median_value = data.median()\n",
    "data_imputed = np.where(outliers, median_value, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a281f0",
   "metadata": {},
   "source": [
    "Binning Data: Mengelompokkan data menjadi interval atau bin untuk mengurangi dampak outlier. Binning dapat membantu meredakan efek ekstrem dari nilai outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning data untuk mengurangi dampak outlier\n",
    "data_binned = pd.cut(data, bins=[0, 30, 60, 100], labels=['Low', 'Medium', 'High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e7256",
   "metadata": {},
   "source": [
    "Penggunaan Model Statistik Robust:Menggunakan model statistik yang lebih tahan terhadap outlier, seperti regresi robust atau metode statistik lain yang tidak terlalu dipengaruhi oleh nilai outlier. from statsmodels.robust.robust_linear_model import RLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RLM(y, X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de2cd5",
   "metadata": {},
   "source": [
    "Menggunakan Metode Deteksi Outlier yang Lebih Toleran: Menggunakan metode deteksi outlier yang lebih toleran, seperti DBSCAN atau Isolation Forest, yang secara intrinsik mampu mengatasi outlier.\n",
    "\n",
    "Pilihan strategi tergantung pada karakteristik data, tujuan analisis, dan konsekuensi dari tindakan yang diambil. Perlu diingat bahwa menangani outlier adalah keputusan yang penting dan harus dilakukan dengan pertimbangan yang matang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan DBSCAN untuk deteksi outlier\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
    "outliers = (dbscan.fit_predict(data) == -1)\n",
    "data_no_outliers = data[~outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb0fbd",
   "metadata": {},
   "source": [
    "# Transformasi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1bff9",
   "metadata": {},
   "source": [
    "* Scaling Data: Data scaling (skala data) adalah suatu proses mengubah rentang nilai dari suatu fitur (kolom) dalam dataset sehingga nilai-nilai tersebut memiliki skala yang serupa. Hal ini dilakukan untuk menghindari masalah ketidakseimbangan skala antar fitur, yang dapat mempengaruhi kinerja beberapa algoritma machine learning. Mengapa Data Scaling Penting: 1. Konsistensi Skala: Algoritma machine learning, seperti regresi linear dan k-nearest neighbors, berdasarkan jarak atau perbedaan antara nilai-nilai fitur. Jika skala fitur berbeda-beda, fitur dengan skala besar akan memberikan kontribusi lebih besar pada perhitungan; 2. Algoritma yang menggunakan optimasi seperti gradient descent akan konvergen lebih cepat jika skala data seragam. Ini meminimalkan waktu pelatihan model.\n",
    "\n",
    "* Normalisasi: Normalisasi data adalah suatu proses yang bertujuan untuk mengubah distribusi data menjadi distribusi normal dengan cara mengubah observasi. Tujuan utama normalisasi adalah untuk menghindari masalah yang mungkin muncul akibat perbedaan skala antar fitur dalam dataset. Mengapa Normalisasi Penting: 1. Perbandingan Seragam: Normalisasi memastikan bahwa setiap fitur memiliki dampak yang setara terhadap analisis, terlepas dari perbedaan skala awal; 2. Kinerja Model yang Lebih Baik: Beberapa algoritma machine learning, seperti k-nearest neighbors dan neural networks, sangat dipengaruhi oleh perbedaan skala. Normalisasi membantu algoritma konvergen lebih cepat dan meningkatkan kinerja model.\n",
    "\n",
    "Normalisasi data adalah langkah penting dalam analisis data dan pemrosesan pra-model untuk memastikan bahwa nilai-nilai fitur memiliki skala seragam. Min-Max Scaling adalah salah satu metode normalisasi yang umum digunakan, tetapi terdapat metode normalisasi lainnya seperti Z-score normalization, Robust scaling, dll. yang juga dapat digunakan sesuai dengan kebutuhan dan karakteristik data Anda. Pemahaman yang baik tentang normalisasi akan membantu Anda menghindari masalah akibat perbedaan skala dalam analisis data dan model machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1abd1b",
   "metadata": {},
   "source": [
    "Jenis-Jenis Normalisasi:\n",
    "* Min-Max Scaling: 1. Deskripsi: Mengubah nilai-nilai dalam rentang 0 hingga 1; 2. Formula: Xnormalized​=X−min(X)/max(X)−min(X)\n",
    "* Z-Score Normalization (Standardization): 1. Deskripsi: Mengubah nilai-nilai sehingga memiliki rata-rata 0 dan deviasi standar; 2. Formula: Xnormalized​=X−mean(X)/std(X).\n",
    "* Robust Scaling: 1. Deskripsi: Mengubah nilai-nilai ke dalam rentang yang lebih tahan terhadap outlier; 2. Formula: Xnormalized​=X−median(X)/IQR(X).\n",
    "* Unit Vector Normalization: 1. Deskripsi: Mengubah vektor fitur ke dalam vektor dengan panjang 1 (unit vector); 2. Formula: Xnormalized​=X/∥X∥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3765063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaling:\n",
      "     Fitur_A   Fitur_B\n",
      "0  0.000000  0.000000\n",
      "1  0.333333  0.333333\n",
      "2  0.666667  0.666667\n",
      "3  1.000000  1.000000\n",
      "\n",
      "Z-Score Normalization:\n",
      "     Fitur_A   Fitur_B\n",
      "0 -1.161895 -1.161895\n",
      "1 -0.387298 -0.387298\n",
      "2  0.387298  0.387298\n",
      "3  1.161895  1.161895\n",
      "\n",
      "Robust Scaling:\n",
      "     Fitur_A   Fitur_B\n",
      "0 -1.000000 -1.000000\n",
      "1 -0.333333 -0.333333\n",
      "2  0.333333  0.333333\n",
      "3  1.000000  1.000000\n",
      "\n",
      "Unit Vector Normalization:\n",
      "     Fitur_A   Fitur_B\n",
      "0  0.995037  0.099504\n",
      "1  0.995037  0.099504\n",
      "2  0.995037  0.099504\n",
      "3  0.995037  0.099504\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Buat dataset contoh\n",
    "data = {'Fitur_A': [10, 20, 30, 40],\n",
    "        'Fitur_B': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "# Min-Max Scaling\n",
    "minmax_scaled_data = (df - df.min()) / (df.max() - df.min())\n",
    "print(\"Min-Max Scaling:\\n\", minmax_scaled_data)\n",
    "# Z-Score Normalization\n",
    "zscore_normalized_data = (df - df.mean()) / df.std()\n",
    "print(\"\\nZ-Score Normalization:\\n\", zscore_normalized_data)\n",
    "# Robust Scaling\n",
    "robust_scaled_data = (df - df.median()) / (df.quantile(0.75) - df.quantile(0.25))\n",
    "print(\"\\nRobust Scaling:\\n\", robust_scaled_data)\n",
    "# Unit Vector Normalization\n",
    "unit_vector_normalized_data = df.apply(lambda x: x / (x**2).sum()**0.5, axis=1)\n",
    "print(\"\\nUnit Vector Normalization:\\n\", unit_vector_normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf817f51",
   "metadata": {},
   "source": [
    "# Encoding data kategorikal.\n",
    "Encoding pada data analisis merujuk pada proses mengubah variabel kategorikal menjadi format numerik agar dapat digunakan dalam analisis data dan pemodelan statistik atau machine learning. Algoritma machine learning umumnya membutuhkan input dalam bentuk numerik, beberapa algoritma dapat salah interpretasi urutan atau hubungan ordinal jika tidak diencode dengan benar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d8f82",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0c6b7",
   "metadata": {},
   "source": [
    "One-Hot Encoding adalah teknik konversi variabel kategorikal menjadi bentuk yang dapat diproses oleh model machine learning tanpa menambahkan informasi ordinal yang tidak ada. Setiap kategori diubah menjadi vektor biner yang merepresentasikan keberadaan atau ketiadaan kategori. Keuntungan menggunakan teknik ini adalah memungkinkan model untuk memahami adanya kategori tanpa memberikan urutan atau tingkatan yang salah. Cara kerja one hot encoding adalah pertama mengubah setiap kategori unik diubah menjadi kolom baru, lalu jkika suatu baris memiliki kategori tersebut, nilai dalam kolom menjadi 1; jika tidak, nilainya menjadi 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2379c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Warna  Warna_Biru  Warna_Hijau  Warna_Kuning  Warna_Merah\n",
      "0   Merah           0            0             0            1\n",
      "1   Hijau           0            1             0            0\n",
      "2    Biru           1            0             0            0\n",
      "3   Merah           0            0             0            1\n",
      "4  Kuning           0            0             1            0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Contoh dataset\n",
    "data = {'Warna': ['Merah', 'Hijau', 'Biru', 'Merah', 'Kuning']}\n",
    "df = pd.DataFrame(data)\n",
    "# One-Hot Encoding dengan Pandas\n",
    "one_hot_encoded = pd.get_dummies(df['Warna'], prefix='Warna')\n",
    "# Gabungkan hasil encoding dengan DataFrame asli\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015dcaf0",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6933d",
   "metadata": {},
   "source": [
    "Label Encoding adalah teknik konversi variabel kategorikal menjadi bentuk numerik dengan memberikan label numerik unik pada setiap kategori. Label diberikan berdasarkan urutan atau frekuensi kemunculan.  Keuntungan Label Encoding adalah sederhana dan mudah diimplementasikan serta cocok untuk variabel kategorikal ordinal. Cara kerja one hot encoding adalah pertama setiap kategori diurutkan atau diberikan label berdasarkan urutan atau frekuensi kemunculan, Setiap nilai kategori digantikan dengan label numerik yang sesuai. Label Encoding dapat dilakukan dengan menggunakan kelas LabelEncoder dari library scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664a8d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Warna  Warna_Encoded\n",
      "0   Merah              3\n",
      "1   Hijau              1\n",
      "2    Biru              0\n",
      "3   Merah              3\n",
      "4  Kuning              2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Contoh dataset\n",
    "data = {'Warna': ['Merah', 'Hijau', 'Biru', 'Merah', 'Kuning']}\n",
    "df = pd.DataFrame(data)\n",
    "# Label Encoding dengan scikit-learn\n",
    "label_encoder = LabelEncoder()\n",
    "df['Warna_Encoded'] = label_encoder.fit_transform(df['Warna'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae1660",
   "metadata": {},
   "source": [
    "# Pertimbangan Pemilihan Teknik Encoding\n",
    "* Tipe Data - Jika variabel kategorikal memiliki tipe ordinal dengan urutan yang bermakna, Label Encoding dapat menjadi pilihan yang baik. Namun, jika variabel bersifat nominal tanpa urutan, One-Hot Encoding sering lebih sesuai.\n",
    "* Jumlah Kategori - One-Hot Encoding cenderung efektif ketika jumlah kategori tidak terlalu banyak. Jika jumlah kategori besar, hal ini dapat menyebabkan peningkatan signifikan dalam dimensi data (curse of dimensionality). Label Encoding dapat menjadi pilihan yang lebih efisien dalam kasus ini.\n",
    "* Tujuan Analisis - Tujuan analisis juga memainkan peran penting dalam pemilihan encoding. Misalnya, jika tujuannya adalah klasifikasi dan variabel kategorikal memiliki tingkatan ordinal, Label Encoding mungkin lebih sesuai. Jika tujuannya adalah regresi atau analisis di mana urutan tidak penting, One-Hot Encoding mungkin lebih baik."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
